---
title: "Week 4 Lab - Sentiment Analysis II"
author: "Clarissa Boyajian"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(pdftools)
library(lubridate) #working with date data
library(tidyverse)
library(tidytext)
library(readr)
library(quanteda)
library(readtext) #quanteda subpackage for reading pdf
library(quanteda.textstats)
library(quanteda.textplots)
library(widyr)# pairwise correlations
library(igraph) #network plots
library(ggraph)
library(here)
```


```{r}
## -- read in, clean, and wrangle data -- ##
files <- list.files(path = here("data/Week5"),
                    pattern = "pdf$", full.names = TRUE)

ej_reports <- lapply(files, pdf_text)

ej_pdf <- readtext(file = here("data/Week5", "*.pdf"), 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "year"),
                   sep = "_")

# creating an initial corpus containing our data
epa_corp <- corpus(x = ej_pdf, text_field = "text" )

# context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops <- tibble(word = c(stop_words$word, more_stops)) 
stop_vec <- as_vector(add_stops)

# convert to tidy format and apply my stop words
raw_text <- tidy(epa_corp)

# Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)

# number of total words by document  
total_words <- raw_words %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

report_words <- left_join(raw_words, total_words)
 
paragraph_tokens <- unnest_tokens(raw_text, 
                            output = paragraphs, input = text, 
                            token = "paragraphs")

paragraph_tokens <- paragraph_tokens %>%
 mutate(par_id = 1:n())

paragraph_words <- unnest_tokens(paragraph_tokens, 
                                  output = word, input = paragraphs, 
                                  token = "words")
```



## Question 1

*What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?*

```{r}
# clean tokens
tokens <- tokens(epa_corp, remove_punct = TRUE) %>% 
  tokens_select(min_nchar = 3) %>% 
  tokens_tolower() %>% 
  tokens_remove(pattern = (stop_vec))
doc_freq_matrix <- dfm(tokens)
```


```{r}
# bigrams
tokens_2 <- tokens_ngrams(tokens, n = 2)
doc_freq_matrix_2 <- dfm(tokens_2) %>% 
  dfm_remove(pattern = c(stop_vec))

freq_words2 <- textstat_frequency(doc_freq_matrix_2, n = 20)
freq_words2$token <- rep("bigram", 20)

freq_words2
```

\newpage

```{r}
# trigrams
tokens_3 <- tokens_ngrams(tokens, n = 3)
doc_freq_matrix_3 <- dfm(tokens_3) %>% 
  dfm_remove(pattern = c(stop_vec))

freq_words_3 <- textstat_frequency(doc_freq_matrix_3, n = 20)
freq_words_3$token <- rep("trigram", 20)

freq_words_3
```

**Answer:** The trigrams appear to be less informative than the bigrams. Many of the trigrams include repetitive information with 6 of the top 10 including the phrase "environmental justice" with another, less inpactful word. Whereas the bigrams seem to include more individual topics, such as "public health", "air quality", and "vulnerable communities". 



## Question 2

*Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!*

```{r}
# word correlations
word_cors <- paragraph_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE)
```


```{r}
# words correlated with "environmental", "health", "equity", and "income"
corr_table_data <- word_cors %>%
  filter(item1 %in% c("environmental", "health", "equity", "income")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item1 = as.factor(item1),
         name = reorder_within(item2, correlation, item1))

ggplot(data = corr_table_data,
       aes(y = name, x = correlation, fill = item1)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~item1, ncol = 2, scales = "free")+
  scale_y_reordered() +
  labs(y = NULL,
       x = NULL,
       title = "Correlations with key words",
       subtitle = "EPA EJ Reports")
```

\newpage

```{r}
health_cors <- word_cors %>% 
  filter(item1 == "health") %>%
  mutate(n = 1:n()) # add column that goes 1 to max rows 
                    # (added to column that is ordered highest to lowest for correlation)

health_cors  %>%
  filter(n <= 50) %>% # get top 50 correlated words
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "goldenrod") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


\newpage

## Question 3

*Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.*

```{r}
keyness_plots <- function(years, target = 1){
  # create corpus based on input files
  files <- list.files(path = here("data/Week5"),
                      pattern = "pdf$", full.names = TRUE)
  
  ej_reports <- lapply(files, pdf_text)
  
  ej_pdf <- readtext(file = here("data/Week5", "*.pdf"), 
                     docvarsfrom = "filenames", 
                     docvarnames = c("type", "year"),
                     sep = "_") %>% 
    filter(docvar3 %in% years)
  
  # creating an initial corpus containing our data
  epa_corp <- corpus(x = ej_pdf, text_field = "text" )
  
  tokens <- tokens(epa_corp, remove_punct = TRUE) %>% 
    tokens_select(min_nchar = 3) %>% 
    tokens_tolower() %>% 
    tokens_remove(pattern = (stop_vec))
  
  doc_freq_matrix <- dfm(tokens)
  
  keyness <- textstat_keyness(doc_freq_matrix,
                              target = target) # target = 1 (refers to first document)
  textplot_keyness(keyness)
}
```


\newpage

```{r}
# keyness plot comparing 2 years
keyness_plots(years = c(2015, 2018), target = 1)
```

\newpage

```{r}
# keyness plot comparing 2 years
keyness_plots(years = c(2015, 2020), target = 1)
```

\newpage

```{r}
# keyness plot comparing 2 years
keyness_plots(years = c(2019, 2020), target = 1)
```

## Question 4

*Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create two objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? [Hint](https://tutorials.quanteda.io/advanced-operations/target-word-collocations/)*

```{r}
term <- c("public", "health", "public health")

tokens_inside <- tokens_keep(tokens, pattern = term, window = 10) %>% 
  tokens_remove(pattern = term) # remove the keywords

tokens_outside <- tokens_remove(tokens, pattern = term, window = 10)
```

```{r}
doc_freq_matrix_inside <- dfm(tokens_inside)
doc_freq_matrix_outside <- dfm(tokens_outside)

tstat_key_inside <- textstat_keyness(rbind(doc_freq_matrix_inside, doc_freq_matrix_outside), 
                                     target = seq_len(ndoc(doc_freq_matrix_inside)))
head(tstat_key_inside, 20)
```

**Answer:** The target is the list of all words within the 10 word window based on the key terms of "public health". And the reference is the list of all other words in the EPA reports.

