---
title: "Lab 1"
author: "Clarissa Boyajian"
date: "4/6/2022"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(here)
library(jsonlite)
library(tidytext)
```


## Lab 1
Query 

```{r}
api_key <- "qTBZr0Mz2Ak8icY1OZTCm0kfxdze9Gnn"
term <- "forest" # use + to string together separate words
begin_date <- "20210401" # YYYYMMDD
end_date <- "20220401" # YYYYMMDD

# construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=", term,
                  "&begin_date=", begin_date, 
                  "&end_date=", end_date,
                  "&facet_filter=true&api-key=", api_key, 
                  sep = "")
```


```{r}
# this code allows for obtaining multiple pages of query results (each search maxes out at 10 pages for NYTimes)
initialQuery <- fromJSON(baseurl)

maxPages <- round((initialQuery$response$meta$hits[1] / 100) - 1) 

pages <- list()
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% 
    data.frame() 
  message("Retrieving page ", i)
  pages[[i + 1]] <- nytSearch 
  Sys.sleep(6) # keeps you from hitting limit for API
}
class(nytSearch)

# need to bind the pages and create a tibble from nytDa
nytDat <- rbind_pages(pages)
```



```{r}
nytDat %>% 
  mutate(pubDay = gsub("T.*", "", response.docs.pub_date)) %>% # replace "T." with "" - remove time but leave dates
  group_by(pubDay) %>%
  summarise(count = n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x = reorder(pubDay, count), 
               y = count), 
           stat = "identity") + 
  coord_flip()
```


```{r}
names(nytDat) # look at variables

paragraph <- names(nytDat)[6] # call the 6th column ("response.doc.lead_paragraph")

# convert from text to `tidytext` format
tokenized <- nytDat %>%
  unnest_tokens(word, paragraph) # here: take paragraphs in and un-nest to word level (1 row for each word in paragraph)

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>% # illegible with all the words displayed
  mutate(word = reorder(word, n))

tokenized <- tokenized %>%
  anti_join(stop_words) # remove all rows that match a stopword

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL,
       x = "Number of times word appears")
```


```{r}
#inspect the list of tokens (words)
tokenized$word # still have not useful tokens

clean_tokens <- str_replace_all(string = tokenized$word,
                                pattern = "land[a-z, A-Z]*", 
                                replacement = "land") # stem tribe words
clean_tokens <- str_remove_all(string = clean_tokens, 
                               pattern = "[:digit:]") # remove all numbers
clean_tokens <- str_remove_all(string = clean_tokens, 
                               pattern = "washington")
clean_tokens <- gsub("â€™s", '', clean_tokens)

tokenized$clean <- clean_tokens # put the cleaned tokens into the `tokenized` df `clean` column

tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>% # illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```


```{r}
# remove the empty strings
tib <-subset(tokenized, clean != "")

# reassign
tokenized <- tib

#try again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>% # illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```


